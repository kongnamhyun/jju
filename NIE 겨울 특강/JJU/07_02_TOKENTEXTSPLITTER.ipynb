{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMvWu9yr7vy75c5Ya/YRMq9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install langchain\n"],"metadata":{"id":"mZpHww7fb_Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","\n","# 파일에서 텍스트를 읽고 주제별로 분할하는 함수\n","def split_text_into_chunks(file_path, chunk_size=500, chunk_overlap=50):\n","    \"\"\"\n","    주어진 파일에서 텍스트를 읽고, TEXTSPLITTER를 사용하여 주제별로 나눕니다.\n","\n","    Parameters:\n","        file_path (str): 읽어올 파일 경로\n","        chunk_size (int): 각 블록의 최대 크기\n","        chunk_overlap (int): 인접한 블록 간의 중복되는 부분 크기\n","\n","    Returns:\n","        list: 분할된 텍스트 블록들의 리스트\n","    \"\"\"\n","    # 파일 읽기\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        file_content = f.read()\n","\n","    # 텍스트 분할기 설정\n","    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","    split_text = text_splitter.split_text(file_content)\n","\n","    return split_text\n","\n","# 파일 경로 설정\n","file_path = \"/content/appendix-keywords.txt\"\n","\n","# 텍스트 분할\n","chunks = split_text_into_chunks(file_path, chunk_size=500, chunk_overlap=50)\n","\n","# 결과 출력\n","for idx, chunk in enumerate(chunks):\n","    print(chunk)\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")\n","\n"],"metadata":{"id":"ZwDECEaEcAYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# API 키를 직접 입력\n","api_key = \"AIzaSyAFtlXiEWy_2Jc_5nf-dahslO0Ov9RmMv0\"\n","genai.configure(api_key=api_key)\n","\n","# 모델 설정\n","generation_config = {\n","  \"temperature\": 1,\n","  \"top_p\": 0.95,\n","  \"top_k\": 40,\n","  \"max_output_tokens\": 8192,\n","  \"response_mime_type\": \"text/plain\",  # 필요에 따라 수정 가능\n","}\n","\n","model = genai.GenerativeModel(\n","  model_name=\"gemini-2.0-flash-exp\",\n","  generation_config=generation_config,\n",")\n","\n","# 채팅 세션 시작\n","chat_session = model.start_chat(\n","  history=[]  # 대화 이력을 여기에 넣을 수 있습니다\n",")\n","\n","# 메시지 전송 및 응답 받기\n","response = chat_session.send_message(\"\")\n","\n","# 응답 출력\n","print(response.text)\n"],"metadata":{"id":"Uejvp5aG44ig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i62wRU5g6i-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WAP4y9Um6jKV"},"execution_count":null,"outputs":[]}]}