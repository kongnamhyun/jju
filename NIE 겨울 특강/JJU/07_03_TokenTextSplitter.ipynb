{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOwzGvC0+s6r7+UmuAJNtBS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TokenTextspliter\n","\n","- 언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야합니다.\n","- TokenTextSplitter 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다."],"metadata":{"id":"vybNsmh77f6X"}},{"cell_type":"code","source":["with open(\"/content/appendix-keywords.txt\") as f:\n","  file = f.read()"],"metadata":{"id":"z2GXKIjS8ydU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(file[:500])"],"metadata":{"id":"SyuDQc0w89eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -q tiktoken"],"metadata":{"id":"kqYxTbLN9gCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_text_splitters import CharacterTextSplitter\n","\n","text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n","    # 청크 크기를 300으로 설정합니다.\n","    chunk_size = 300,\n","    # 청크 간 중복되는 부분이 없도록 설정합니다.\n","    chunk_overlap=0,\n",")\n","#file 텍스트를 청크 단위로 분활합니다.\n","texts = text_splitter.split_text(file)"],"metadata":{"id":"rrOpRnln9gIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(texts))"],"metadata":{"id":"rYb3h72u-r2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# texts 리스트의 첫 번째 요소를 출력합니다.\n","print(texts[0])"],"metadata":{"id":"U8hhZKZN_V_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokentextsplitter"],"metadata":{"id":"kX3Xv2SP_5zX"}},{"cell_type":"code","source":["from langchain_text_splitters import TokenTextSplitter\n","\n","text_splitter = TokenTextSplitter(\n","    chunk_size=200, #청크 크기를 10으로 고정합니다.\n","    chunk_overlap=0,\n",")\n","\n","# state_of_the_union 텍스트를 청크로 분활합니다.\n","texts = text_splitter.split_text(file)\n","print(texts[0]) # 분활된 텍스트의 첫 번째 청크를 출력합니다."],"metadata":{"id":"uL7fd0Dg_pTO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#spaCy\n","\n","spaCy는python과Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.\n","NLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.\n","1. 텍스트가 분활되는 방식:spaCy tokenizer에 의해 분활됩니다.\n","2. chunk size가 측정되는 방법:문자 수를"],"metadata":{"id":"zzvp-RqTB3cv"}},{"cell_type":"code","source":["!pip install  -q spacy"],"metadata":{"id":"THNuH-q4CdCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm --quiet"],"metadata":{"id":"yt-OTbrNCnyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"55xYVFcNCjRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","from langchain_text_splitters import SpacyTextSplitter\n","\n","# 경고 메시지를 무시\n","warnings.filterwarnings(\"ignore\")\n","\n","# SpacyTextSplitter를 생성\n","text_splitter = SpacyTextSplitter(\n","    chunk_size=200,  # 청크 크기를 200으로 설정합니다.\n","    chunk_overlap=50,  # 청크 간 중복을 50으로 설정합니다.\n",")"],"metadata":{"id":"X0LBC_i8CuIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text_splitter를 사용하여 file텍스트를 분활합니다.\n","texts = text_splitter.split_text(file)\n","print(texts[1]) # 분활된 텍스트의 첫 번째 요소를 출력합니다."],"metadata":{"id":"K4I-vS5uDSRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SentenceTransformers\n","\n","SentenceTransformersTokenTextSplitter는 sentence-transformer 모델에 특화된 텍스트 분할기입니다.\n","\n","기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다."],"metadata":{"id":"ttKzwnV7G4Z8"}},{"cell_type":"code","source":["from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n","\n","# 문장 분할기를 생성하고 청크 간 중복을 0으로 설정합니다.\n","splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)"],"metadata":{"id":"i7rqgFf6G_Dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n","with open(\"/content/appendix-keywords.txt\") as f:\n","    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n","\n","# 파일으로부터 읽은 내용을 일부 출력합니다.\n","print(file[:350])"],"metadata":{"id":"LyGS4EbCHPQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_start_and_stop_tokens = 2  # 시작과 종료 토큰의 개수를 2로 설정합니다.\n","\n","# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수를 뺍니다.\n","text_token_count = splitter.count_tokens(\n","    text=file) - count_start_and_stop_tokens\n","print(text_token_count)  # 계산된 텍스트 토큰 개수를 출력합니다."],"metadata":{"id":"MjJ60du8IJG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["splitter.split_text() 함수를 사용하여 text_to_split 변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다."],"metadata":{"id":"-6WNbwxwIx8y"}},{"cell_type":"code","source":["text_chunks = splitter.split_text(text=file)  # 텍스트를 청크로 분할합니다."],"metadata":{"id":"zy9X5TkIIjBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 0번째 청크를 출력합니다.\n","print(text_chunks[1])  # 분할된 텍스트 청크 중 두 번째 청크를 출력합니다."],"metadata":{"id":"xqzYigzJIqyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# KoNLPy\n","KoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.\n","\n","토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.\n","\n","이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.\n","\n","영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.\n","\n","토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.\n","\n","영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다.\n","\n","KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할\n","한국어 텍스트의 경우 KoNLPY에는 Kkma(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.\n","\n","Kkma는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.\n","\n","문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.\n","\n","텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다.\n","\n","사용시 고려사항\n","Kkma는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서 Kkma는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.\n","\n","- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.\n","- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다."],"metadata":{"id":"Gfbi4o3eJtps"}},{"cell_type":"code","source":["%pip install -qU konlpy"],"metadata":{"id":"8JTRBwm_J3la"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n","with open(\"/content/appendix-keywords.txt\") as f:\n","    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n","\n","# 파일으로부터 읽은 내용을 일부 출력합니다.\n","print(file[:350])"],"metadata":{"id":"zS9Wj500KC23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import chunk\n","from langchain_text_splitters import KonlpyTextSplitter\n","\n","# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체를 생성합니다.\n","text_splitter = KonlpyTextSplitter(chunk_size=200, chunk_overlap=50)"],"metadata":{"id":"nhQxZBvKKeJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = text_splitter.split_text(file)  # 한국어 문서를 문장 단위로 분할합니다.\n","print(texts[0])  # 분할된 문장 중 첫 번째 문장을 출력합니다."],"metadata":{"id":"bpsl_E-2KxgC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Hugging Face tokenizer\n","Hugging Face는 다양한 토크나이저를 제공합니다.\n","\n","이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.\n","\n","텍스트 분할 방식은 다음과 같습니다:\n","\n","전달된 문자 단위로 분할됩니다.\n","청크 크기 측정 방식은 다음과 같습니다:\n","\n","- Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.\n","\n","- GPT2TokenizerFast 클래스를 사용하여 tokenizer 객체를 생성합니다.\n","- from_pretrained 메서드를 호출하여 사전 학습된 \"gpt2\" 토크나이저 모델을 로드합니다."],"metadata":{"id":"1GV3HooOLlNu"}},{"cell_type":"code","source":["from transformers import GPT2TokenizerFast\n","\n","# GPT-2 모델의 토크나이저를 불러옵니다.\n","hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"],"metadata":{"id":"3wLWQpBaLx_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n","with open(\"/content/appendix-keywords.txt\") as f:\n","    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n","\n","# 파일으로부터 읽은 내용을 일부 출력합니다.\n","print(file[:350])"],"metadata":{"id":"oVwPoAZdLuom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n","    # 허깅페이스 토크나이저를 사용하여 CharacterTextSplitter 객체를 생성합니다.\n","    hf_tokenizer,\n","    chunk_size=300,\n","    chunk_overlap=50,\n",")\n","# state_of_the_union 텍스트를 분할하여 texts 변수에 저장합니다.\n","texts = text_splitter.split_text(file)"],"metadata":{"id":"TTa1IYSxL-DF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(texts[0]) # texts 리스트의 1번째 요소를 출력합니다."],"metadata":{"id":"pPymMT_EMTnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# 파일에서 키워드를 추출하고 포맷팅하는 함수\n","def format_keywords_from_file(file_path, target_titles):\n","    \"\"\"\n","    주어진 파일에서 특정 키워드만 추출하고 구분선을 추가하여 출력합니다.\n","\n","    Parameters:\n","        file_path (str): 읽어올 파일 경로\n","        target_titles (list): 출력할 키워드 제목 리스트\n","\n","    Returns:\n","        str: 변환된 텍스트\n","    \"\"\"\n","    # 파일 읽기\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        file_content = f.read()\n","\n","    # 블록 분리\n","    blocks = re.split(r'(?<=\\n)(?=[A-Z][^\\n]*\\n)', file_content.strip())\n","\n","    # 선택된 키워드만 변환\n","    selected_blocks = []\n","    for block in blocks:\n","        title = block.split(\"\\n\", 1)[0].strip()\n","        if title.upper() in target_titles:\n","            selected_blocks.append(block.strip())\n","\n","    # 블록 사이에 구분선 추가\n","    return \"\\n\\n----------------------------------------\\n\\n\".join(selected_blocks)\n","\n","# 파일 경로와 출력할 키워드 목록\n","file_path = \"/content/appendix-keywords.txt\"\n","target_titles = [\"SEMANTIC SEARCH\", \"EMBEDDING\", \"TOKEN\"]\n","\n","# 변환된 결과 생성\n","formatted_text = format_keywords_from_file(file_path, target_titles)\n","\n","# 결과 출력\n","print(formatted_text)\n"],"metadata":{"id":"UeqaM7_0Mcn9"},"execution_count":null,"outputs":[]}]}